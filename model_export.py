# ==============================
# MODEL EXPORT
# Modeli FarklÄ± Formatlara DÄ±ÅŸa Aktar
# ==============================

import tensorflow as tf
import numpy as np
from pathlib import Path
from typing import Union
import config
import utils

logger = utils.setup_logging(__name__)

# ==============================
# MODEL EXPORTER
# ==============================

class ModelExporter:
    """Model dÄ±ÅŸa aktarma aracÄ±"""
    
    def __init__(self, model_path: Union[str, Path]):
        """
        Args:
            model_path: Model dosya yolu
        """
        self.model_path = Path(model_path)
        
        if not self.model_path.exists():
            raise FileNotFoundError(f"Model bulunamadÄ±: {model_path}")
        
        logger.info(f"Model yÃ¼kleniyor: {model_path}")
        self.model = utils.load_model_safe(model_path)
        
        self.export_dir = config.MODELS_DIR / "exported"
        self.export_dir.mkdir(exist_ok=True, parents=True)
    
    def export_h5(self, output_path: Union[str, Path] = None):
        """HDF5 formatÄ±na aktar"""
        if output_path is None:
            output_path = self.export_dir / f"{self.model_path.stem}.h5"
        
        logger.info(f"H5 formatÄ±na aktarÄ±lÄ±yor: {output_path}")
        
        try:
            self.model.save(output_path, save_format='h5')
            logger.info(f"âœ… H5 export baÅŸarÄ±lÄ±: {output_path}")
            return output_path
        except Exception as e:
            logger.error(f"âŒ H5 export hatasÄ±: {e}")
            raise
    
    def export_savedmodel(self, output_path: Union[str, Path] = None):
        """TensorFlow SavedModel formatÄ±na aktar"""
        if output_path is None:
            output_path = self.export_dir / f"{self.model_path.stem}_savedmodel"
        
        logger.info(f"SavedModel formatÄ±na aktarÄ±lÄ±yor: {output_path}")
        
        try:
            tf.saved_model.save(self.model, str(output_path))
            logger.info(f"âœ… SavedModel export baÅŸarÄ±lÄ±: {output_path}")
            return output_path
        except Exception as e:
            logger.error(f"âŒ SavedModel export hatasÄ±: {e}")
            raise
    
    def export_tflite(
        self,
        output_path: Union[str, Path] = None,
        quantize: bool = True
    ):
        """
        TensorFlow Lite formatÄ±na aktar
        
        Args:
            output_path: Ã‡Ä±ktÄ± yolu
            quantize: Quantization uygula (model boyutunu kÃ¼Ã§Ã¼ltÃ¼r)
        """
        if output_path is None:
            suffix = "_quantized" if quantize else ""
            output_path = self.export_dir / f"{self.model_path.stem}{suffix}.tflite"
        
        logger.info(f"TFLite formatÄ±na aktarÄ±lÄ±yor: {output_path}")
        logger.info(f"Quantization: {'Evet' if quantize else 'HayÄ±r'}")
        
        try:
            # Converter oluÅŸtur
            converter = tf.lite.TFLiteConverter.from_keras_model(self.model)
            
            if quantize:
                # Dynamic range quantization
                converter.optimizations = [tf.lite.Optimize.DEFAULT]
                
                # Float16 quantization (daha iyi performans)
                converter.target_spec.supported_types = [tf.float16]
            
            # Convert
            tflite_model = converter.convert()
            
            # Kaydet
            with open(output_path, 'wb') as f:
                f.write(tflite_model)
            
            # Boyut bilgisi
            original_size = self.model_path.stat().st_size / (1024 * 1024)  # MB
            tflite_size = len(tflite_model) / (1024 * 1024)  # MB
            
            logger.info(f"âœ… TFLite export baÅŸarÄ±lÄ±: {output_path}")
            logger.info(f"Orijinal boyut: {original_size:.2f} MB")
            logger.info(f"TFLite boyut: {tflite_size:.2f} MB")
            logger.info(f"SÄ±kÄ±ÅŸtÄ±rma: {(1 - tflite_size/original_size)*100:.1f}%")
            
            return output_path
        
        except Exception as e:
            logger.error(f"âŒ TFLite export hatasÄ±: {e}")
            raise
    
    def export_onnx(self, output_path: Union[str, Path] = None):
        """
        ONNX formatÄ±na aktar
        
        Not: tf2onnx kÃ¼tÃ¼phanesi gereklidir
        """
        if output_path is None:
            output_path = self.export_dir / f"{self.model_path.stem}.onnx"
        
        logger.info(f"ONNX formatÄ±na aktarÄ±lÄ±yor: {output_path}")
        
        try:
            import tf2onnx
            
            # Model spec
            spec = (tf.TensorSpec(self.model.input_shape, tf.float32, name="input"),)
            
            # Convert
            model_proto, _ = tf2onnx.convert.from_keras(
                self.model,
                input_signature=spec,
                opset=13
            )
            
            # Kaydet
            with open(output_path, 'wb') as f:
                f.write(model_proto.SerializeToString())
            
            logger.info(f"âœ… ONNX export baÅŸarÄ±lÄ±: {output_path}")
            return output_path
        
        except ImportError:
            logger.error("âŒ tf2onnx kÃ¼tÃ¼phanesi yÃ¼klÃ¼ deÄŸil!")
            logger.info("YÃ¼klemek iÃ§in: pip install tf2onnx")
            raise
        
        except Exception as e:
            logger.error(f"âŒ ONNX export hatasÄ±: {e}")
            raise
    
    def export_all(self, quantize_tflite: bool = True):
        """TÃ¼m formatlara aktar"""
        logger.info(f"\n{'='*60}")
        logger.info("TÃœM FORMATLARA AKTARILIYOR")
        logger.info(f"{'='*60}")
        
        results = {}
        
        # H5
        try:
            results['h5'] = self.export_h5()
        except Exception as e:
            results['h5'] = f"Hata: {e}"
        
        # SavedModel
        try:
            results['savedmodel'] = self.export_savedmodel()
        except Exception as e:
            results['savedmodel'] = f"Hata: {e}"
        
        # TFLite
        try:
            results['tflite'] = self.export_tflite(quantize=quantize_tflite)
        except Exception as e:
            results['tflite'] = f"Hata: {e}"
        
        # ONNX
        try:
            results['onnx'] = self.export_onnx()
        except Exception as e:
            results['onnx'] = f"Hata: {e}"
        
        # Ã–zet
        logger.info(f"\n{'='*60}")
        logger.info("EXPORT Ã–ZET")
        logger.info(f"{'='*60}")
        
        for format_name, result in results.items():
            if isinstance(result, Path):
                logger.info(f"âœ… {format_name.upper()}: {result}")
            else:
                logger.info(f"âŒ {format_name.upper()}: {result}")
        
        return results

# ==============================
# TFLITE INFERENCE TEST
# ==============================

def test_tflite_model(
    tflite_path: Union[str, Path],
    test_image_path: Union[str, Path]
):
    """TFLite modelini test et"""
    logger.info(f"\n{'='*60}")
    logger.info("TFLITE MODEL TESTÄ°")
    logger.info(f"{'='*60}")
    
    # TFLite interpreter
    interpreter = tf.lite.Interpreter(model_path=str(tflite_path))
    interpreter.allocate_tensors()
    
    # Input/output detaylarÄ±
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()
    
    logger.info(f"Input shape: {input_details[0]['shape']}")
    logger.info(f"Output shape: {output_details[0]['shape']}")
    
    # Test gÃ¶rÃ¼ntÃ¼sÃ¼
    img_array = utils.load_and_preprocess_image(test_image_path)
    
    # Tahmin
    interpreter.set_tensor(input_details[0]['index'], img_array.astype(np.float32))
    interpreter.invoke()
    
    predictions = interpreter.get_tensor(output_details[0]['index'])[0]
    
    # SonuÃ§
    top_idx = np.argmax(predictions)
    top_class = config.CLASS_NAMES[top_idx]
    confidence = predictions[top_idx]
    
    logger.info(f"\nTahmin: {top_class}")
    logger.info(f"GÃ¼ven: {confidence:.4f}")
    
    return top_class, confidence

# ==============================
# MAIN EXECUTION
# ==============================

if __name__ == "__main__":
    print("=" * 60)
    print("MODEL EXPORT ARACI")
    print("=" * 60)
    
    # Model yolu
    model_path = config.MODELS_DIR / "eurosat_best_model.keras"
    
    if not model_path.exists():
        print(f"\nâŒ Model bulunamadÄ±: {model_path}")
        print("Ã–nce modeli eÄŸitin!")
        exit(1)
    
    # Exporter oluÅŸtur
    exporter = ModelExporter(model_path)
    
    # TÃ¼m formatlara aktar
    results = exporter.export_all(quantize_tflite=True)
    
    # TFLite test
    if 'tflite' in results and isinstance(results['tflite'], Path):
        print("\n" + "=" * 60)
        print("TFLITE MODEL TEST EDÄ°LÄ°YOR")
        print("=" * 60)
        
        # Test gÃ¶rÃ¼ntÃ¼sÃ¼
        image_files = utils.get_image_files(config.DATA_DIR)
        if image_files:
            test_image = np.random.choice(image_files)
            print(f"Test gÃ¶rÃ¼ntÃ¼sÃ¼: {test_image}")
            
            try:
                test_tflite_model(results['tflite'], test_image)
            except Exception as e:
                print(f"âŒ Test hatasÄ±: {e}")
    
    print("\nâœ… Model export tamamlandÄ±!")
    print(f"ğŸ“ Export dizini: {exporter.export_dir}")
